{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/activus/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "# Importing Modules\n",
    "from sklearn import datasets\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL & Unzip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions download -c tabular-playground-series-jul-2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip tabular-playground-series-jul-2022.zip -d data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm tabular-playground-series-jul-2022.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_00</th>\n",
       "      <th>f_01</th>\n",
       "      <th>f_02</th>\n",
       "      <th>f_03</th>\n",
       "      <th>f_04</th>\n",
       "      <th>f_05</th>\n",
       "      <th>f_06</th>\n",
       "      <th>f_07</th>\n",
       "      <th>f_08</th>\n",
       "      <th>f_09</th>\n",
       "      <th>...</th>\n",
       "      <th>f_19</th>\n",
       "      <th>f_20</th>\n",
       "      <th>f_21</th>\n",
       "      <th>f_22</th>\n",
       "      <th>f_23</th>\n",
       "      <th>f_24</th>\n",
       "      <th>f_25</th>\n",
       "      <th>f_26</th>\n",
       "      <th>f_27</th>\n",
       "      <th>f_28</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.389420</td>\n",
       "      <td>-0.912791</td>\n",
       "      <td>0.648951</td>\n",
       "      <td>0.589045</td>\n",
       "      <td>-0.830817</td>\n",
       "      <td>0.733624</td>\n",
       "      <td>2.258560</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.478412</td>\n",
       "      <td>-0.757002</td>\n",
       "      <td>-0.763635</td>\n",
       "      <td>-1.090369</td>\n",
       "      <td>1.142641</td>\n",
       "      <td>-0.884274</td>\n",
       "      <td>1.137896</td>\n",
       "      <td>1.309073</td>\n",
       "      <td>1.463002</td>\n",
       "      <td>0.813527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.689249</td>\n",
       "      <td>-0.453954</td>\n",
       "      <td>0.654175</td>\n",
       "      <td>0.995248</td>\n",
       "      <td>-1.653020</td>\n",
       "      <td>0.863810</td>\n",
       "      <td>-0.090651</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.428791</td>\n",
       "      <td>-0.089908</td>\n",
       "      <td>-1.784204</td>\n",
       "      <td>-0.839474</td>\n",
       "      <td>0.459685</td>\n",
       "      <td>1.759412</td>\n",
       "      <td>-0.275422</td>\n",
       "      <td>-0.852168</td>\n",
       "      <td>0.562457</td>\n",
       "      <td>-2.680541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.809079</td>\n",
       "      <td>0.324568</td>\n",
       "      <td>-1.170602</td>\n",
       "      <td>-0.624491</td>\n",
       "      <td>0.105448</td>\n",
       "      <td>0.783948</td>\n",
       "      <td>1.988301</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.413534</td>\n",
       "      <td>-1.602377</td>\n",
       "      <td>1.190984</td>\n",
       "      <td>3.267116</td>\n",
       "      <td>-0.088322</td>\n",
       "      <td>-2.168635</td>\n",
       "      <td>-0.974989</td>\n",
       "      <td>1.335763</td>\n",
       "      <td>-1.110655</td>\n",
       "      <td>-3.630723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.500923</td>\n",
       "      <td>0.229049</td>\n",
       "      <td>0.264109</td>\n",
       "      <td>0.231520</td>\n",
       "      <td>0.415012</td>\n",
       "      <td>-1.221269</td>\n",
       "      <td>0.138850</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.619283</td>\n",
       "      <td>1.287801</td>\n",
       "      <td>0.532837</td>\n",
       "      <td>1.036631</td>\n",
       "      <td>-2.041828</td>\n",
       "      <td>1.440490</td>\n",
       "      <td>-1.900191</td>\n",
       "      <td>-0.630771</td>\n",
       "      <td>-0.050641</td>\n",
       "      <td>0.238333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.671268</td>\n",
       "      <td>-1.039533</td>\n",
       "      <td>-0.270155</td>\n",
       "      <td>-1.830264</td>\n",
       "      <td>-0.290108</td>\n",
       "      <td>-1.852809</td>\n",
       "      <td>0.781898</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.628830</td>\n",
       "      <td>-0.434948</td>\n",
       "      <td>0.322505</td>\n",
       "      <td>0.284326</td>\n",
       "      <td>-2.438365</td>\n",
       "      <td>1.473930</td>\n",
       "      <td>-1.044684</td>\n",
       "      <td>1.602686</td>\n",
       "      <td>-0.405263</td>\n",
       "      <td>-1.987263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97995</th>\n",
       "      <td>0.237591</td>\n",
       "      <td>1.657034</td>\n",
       "      <td>-0.689282</td>\n",
       "      <td>0.313710</td>\n",
       "      <td>-0.299039</td>\n",
       "      <td>0.329139</td>\n",
       "      <td>1.607378</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.290116</td>\n",
       "      <td>-0.258141</td>\n",
       "      <td>-0.973640</td>\n",
       "      <td>1.369508</td>\n",
       "      <td>0.391055</td>\n",
       "      <td>2.152426</td>\n",
       "      <td>-0.208944</td>\n",
       "      <td>-1.475403</td>\n",
       "      <td>0.298448</td>\n",
       "      <td>0.445039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97996</th>\n",
       "      <td>0.322696</td>\n",
       "      <td>0.710411</td>\n",
       "      <td>0.562625</td>\n",
       "      <td>-1.321713</td>\n",
       "      <td>-0.357708</td>\n",
       "      <td>0.182024</td>\n",
       "      <td>0.178558</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117687</td>\n",
       "      <td>1.388242</td>\n",
       "      <td>0.342400</td>\n",
       "      <td>1.680537</td>\n",
       "      <td>-0.860409</td>\n",
       "      <td>0.579165</td>\n",
       "      <td>1.162692</td>\n",
       "      <td>0.134994</td>\n",
       "      <td>0.994666</td>\n",
       "      <td>0.727642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97997</th>\n",
       "      <td>-0.249364</td>\n",
       "      <td>-0.459545</td>\n",
       "      <td>1.886122</td>\n",
       "      <td>-1.340310</td>\n",
       "      <td>0.195029</td>\n",
       "      <td>-0.559520</td>\n",
       "      <td>-0.379767</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.850223</td>\n",
       "      <td>-1.787648</td>\n",
       "      <td>-1.268115</td>\n",
       "      <td>-1.508330</td>\n",
       "      <td>1.945622</td>\n",
       "      <td>1.503645</td>\n",
       "      <td>0.194968</td>\n",
       "      <td>2.142693</td>\n",
       "      <td>1.646042</td>\n",
       "      <td>0.641466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97998</th>\n",
       "      <td>0.311408</td>\n",
       "      <td>2.185237</td>\n",
       "      <td>0.761367</td>\n",
       "      <td>0.436723</td>\n",
       "      <td>0.464967</td>\n",
       "      <td>0.062321</td>\n",
       "      <td>-0.334025</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010839</td>\n",
       "      <td>1.118906</td>\n",
       "      <td>1.565765</td>\n",
       "      <td>0.358480</td>\n",
       "      <td>0.547615</td>\n",
       "      <td>1.224439</td>\n",
       "      <td>-0.537998</td>\n",
       "      <td>-1.610954</td>\n",
       "      <td>-0.616227</td>\n",
       "      <td>-0.066211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97999</th>\n",
       "      <td>0.755170</td>\n",
       "      <td>0.567483</td>\n",
       "      <td>1.456767</td>\n",
       "      <td>-0.579071</td>\n",
       "      <td>-0.048474</td>\n",
       "      <td>-1.206240</td>\n",
       "      <td>0.784305</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1.180805</td>\n",
       "      <td>-0.925705</td>\n",
       "      <td>-1.368680</td>\n",
       "      <td>-2.465425</td>\n",
       "      <td>1.453582</td>\n",
       "      <td>-1.685122</td>\n",
       "      <td>0.129689</td>\n",
       "      <td>-0.970897</td>\n",
       "      <td>1.404988</td>\n",
       "      <td>-0.711098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98000 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           f_00      f_01      f_02      f_03      f_04      f_05      f_06  \\\n",
       "id                                                                            \n",
       "0     -0.389420 -0.912791  0.648951  0.589045 -0.830817  0.733624  2.258560   \n",
       "1     -0.689249 -0.453954  0.654175  0.995248 -1.653020  0.863810 -0.090651   \n",
       "2      0.809079  0.324568 -1.170602 -0.624491  0.105448  0.783948  1.988301   \n",
       "3     -0.500923  0.229049  0.264109  0.231520  0.415012 -1.221269  0.138850   \n",
       "4     -0.671268 -1.039533 -0.270155 -1.830264 -0.290108 -1.852809  0.781898   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "97995  0.237591  1.657034 -0.689282  0.313710 -0.299039  0.329139  1.607378   \n",
       "97996  0.322696  0.710411  0.562625 -1.321713 -0.357708  0.182024  0.178558   \n",
       "97997 -0.249364 -0.459545  1.886122 -1.340310  0.195029 -0.559520 -0.379767   \n",
       "97998  0.311408  2.185237  0.761367  0.436723  0.464967  0.062321 -0.334025   \n",
       "97999  0.755170  0.567483  1.456767 -0.579071 -0.048474 -1.206240  0.784305   \n",
       "\n",
       "       f_07  f_08  f_09  ...      f_19      f_20      f_21      f_22  \\\n",
       "id                       ...                                           \n",
       "0         2    13    14  ... -0.478412 -0.757002 -0.763635 -1.090369   \n",
       "1         2     3     6  ... -0.428791 -0.089908 -1.784204 -0.839474   \n",
       "2         5    11     5  ... -0.413534 -1.602377  1.190984  3.267116   \n",
       "3         6     2    13  ...  0.619283  1.287801  0.532837  1.036631   \n",
       "4         8     7     5  ... -1.628830 -0.434948  0.322505  0.284326   \n",
       "...     ...   ...   ...  ...       ...       ...       ...       ...   \n",
       "97995     5     7     8  ... -0.290116 -0.258141 -0.973640  1.369508   \n",
       "97996     3     9     2  ...  0.117687  1.388242  0.342400  1.680537   \n",
       "97997     8     9    10  ... -0.850223 -1.787648 -1.268115 -1.508330   \n",
       "97998     1     8    11  ... -0.010839  1.118906  1.565765  0.358480   \n",
       "97999     0    11     3  ...  1.180805 -0.925705 -1.368680 -2.465425   \n",
       "\n",
       "           f_23      f_24      f_25      f_26      f_27      f_28  \n",
       "id                                                                 \n",
       "0      1.142641 -0.884274  1.137896  1.309073  1.463002  0.813527  \n",
       "1      0.459685  1.759412 -0.275422 -0.852168  0.562457 -2.680541  \n",
       "2     -0.088322 -2.168635 -0.974989  1.335763 -1.110655 -3.630723  \n",
       "3     -2.041828  1.440490 -1.900191 -0.630771 -0.050641  0.238333  \n",
       "4     -2.438365  1.473930 -1.044684  1.602686 -0.405263 -1.987263  \n",
       "...         ...       ...       ...       ...       ...       ...  \n",
       "97995  0.391055  2.152426 -0.208944 -1.475403  0.298448  0.445039  \n",
       "97996 -0.860409  0.579165  1.162692  0.134994  0.994666  0.727642  \n",
       "97997  1.945622  1.503645  0.194968  2.142693  1.646042  0.641466  \n",
       "97998  0.547615  1.224439 -0.537998 -1.610954 -0.616227 -0.066211  \n",
       "97999  1.453582 -1.685122  0.129689 -0.970897  1.404988 -0.711098  \n",
       "\n",
       "[98000 rows x 29 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/data.csv', index_col='id')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = StandardScaler()\n",
    "df = std.fit_transform(df)\n",
    "\n",
    "test = torch.tensor(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tens = torch.tensor(df, dtype=torch.float32)\n",
    "dataset_train = TensorDataset(tens)\n",
    "loader_train = DataLoader(dataset_train, batch_size=256, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/activus/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:82: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "n_feature = 29 \n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_feature, n_feature*2)\n",
    "        self.bn1 = nn.BatchNorm1d(n_feature*2)\n",
    "        self.fc2 = nn.Linear(n_feature*2, n_feature*4)\n",
    "        \n",
    "        self.fc3 = nn.Linear(n_feature*4, n_feature*8)\n",
    "        # self.fc4 = nn.Linear(n_feature*8, n_feature*16)\n",
    "        # self.fc5 = nn.Linear(n_feature*16, n_feature*32)\n",
    "        # self.fc6 = nn.Linear(n_feature*32, n_feature*64)\n",
    "        # self.fc7 = nn.Linear(n_feature*64, n_feature*128)\n",
    "        self.bn2 = nn.BatchNorm1d(n_feature*8)\n",
    "        self.fc15 = nn.Linear(n_feature*8, 7)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        x = F.relu(self.fc3(x))\n",
    "        # x = F.relu(self.fc4(x))\n",
    "        # x = F.relu(self.fc5(x))\n",
    "        # x = F.relu(self.fc6(x))\n",
    "        # x = F.relu(self.fc7(x))\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.fc15(x))\n",
    "                \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (fc1): Linear(in_features=29, out_features=58, bias=True)\n",
      "  (bn1): BatchNorm1d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=58, out_features=116, bias=True)\n",
      "  (fc3): Linear(in_features=116, out_features=232, bias=True)\n",
      "  (bn2): BatchNorm1d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc15): Linear(in_features=232, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "# model.apply(init_weights)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "batch_size = 128\n",
    "epochs = 300\n",
    "g = 0.985"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<enumerate at 0x7f235bbcfc40>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enumerate(loader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unsupervised_weights(X, n_hidden, n_epochs, batch_size, \n",
    "        learning_rate=2e-2, precision=1e-30, anti_hebbian_learning_strength=0.4, lebesgue_norm=2.0, rank=2):\n",
    "    sample_sz = X.shape[1]    \n",
    "    weights = torch.rand((n_hidden, sample_sz), dtype=torch.float)  \n",
    "    for epoch in range(n_epochs):    \n",
    "        eps = learning_rate * (1 - epoch / n_epochs)        \n",
    "        shuffled_epoch_data = X[torch.randperm(X.shape[0]),:]\n",
    "        for i in range(X.shape[0] // batch_size):\n",
    "            mini_batch = shuffled_epoch_data[i*batch_size:(i+1)*batch_size,:]           \n",
    "            mini_batch = torch.transpose(torch.tensor(mini_batch, dtype=torch.float32), 0, 1)            \n",
    "            sign = torch.sign(weights)            \n",
    "            W = sign * torch.abs(weights) ** (lebesgue_norm - 1)        \n",
    "            tot_input=torch.mm(W, mini_batch)            \n",
    "            \n",
    "            y = torch.argsort(tot_input, dim=0)            \n",
    "            yl = torch.zeros((n_hidden, batch_size), dtype = torch.float)\n",
    "            yl[y[n_hidden-1,:], torch.arange(batch_size)] = 1.0\n",
    "            yl[y[n_hidden-rank], torch.arange(batch_size)] =- anti_hebbian_learning_strength            \n",
    "                    \n",
    "            xx = torch.sum(yl * tot_input,1)            \n",
    "            xx = xx.unsqueeze(1)                    \n",
    "            xx = xx.repeat(1, sample_sz)                            \n",
    "            ds = torch.mm(yl, torch.transpose(mini_batch, 0, 1)) - xx * weights            \n",
    "            \n",
    "            nc = torch.max(torch.abs(ds))            \n",
    "            if nc < precision: nc = precision            \n",
    "            weights += eps*(ds/nc)\n",
    "            print(nc)\n",
    "            print(precision)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1950425/3896289796.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1950425/1798613367.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Rudimentary outline of training loop\n",
    "for epoch in range(20):\n",
    "    for inputs in loader_train:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
